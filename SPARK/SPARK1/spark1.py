# -*- coding: utf-8 -*-
"""SPARK1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ynzVtiFj0LY_nL5J_8LCmgBnj1P9x1al
"""

!pip install pyspark

import pyspark
from pyspark.sql import SparkSession
spark=SparkSession.builder.appName('Fire DF').getOrCreate()

from pyspark.sql.functions import *
from pyspark.sql.types import *

from google.colab import drive
drive.mount('/content/drive/')

df=spark.read.csv('sf-fire-calls.csv',header=True,inferSchema=True)

df.show()

df=df.withColumn('Date',to_date(col('callDate'),'dd/mm/yyyy')).drop('callDate')

clean_df=df.withColumn('Year',year(col('Date'))).withColumn('Month',month(col('Date'))).withColumn('Week',weekofyear(col('Date')))
clean_df.show()

df.count()

clean_df.select('Year').groupBy('Year').count().orderBy('Year',ascending=True).show()

clean_df.select('callType').where(col('Year')=='2018').distinct().show(truncate=False)

#Which week in the year in 2018 had the most fire calls?
fire_calls_2018 = clean_df.filter(col('Year') == 2018)
max_week = fire_calls_2018.groupBy('Week').count()
max_count = max_week.agg({'count': 'max'}).collect()[0][0]
result = max_week.filter(col('count') == max_count).select('Week')
result.show()

clean_df.groupBy('Year','Month').count().show()

selected_year = 2018
filtered_data = clean_df.filter(col('Year') == selected_year)
monthly_report = filtered_data.groupBy('Month', 'CallType').count()
monthly_report.show()

#consider month and if particular month then estimate the season
#UDF->user defined function to register all the method calls
def mapSeason(data):
  if 2 < data < 6:
    return 'Spring'
  elif 5 < data <9:
    return 'Summer'
  elif 8 < data < 12:
    return 'Autumn'
  else:
    return 'Winter'
seasonUDF=udf(mapSeason,StringType())
clean_df=clean_df.withColumn('Season',seasonUDF(col('Month')))
clean_df.show()

from pyspark.sql.window import Window
from pyspark.sql.functions import rank
filtered_data = clean_df.filter(col('Year') == 2018)
window_spec = Window.partitionBy('Season').orderBy(col('count').desc())
ranked_data = filtered_data.groupBy('Season', 'CallType').count().withColumn("rank", rank().over(window_spec))
top_five_in_each_season = ranked_data.filter(col('rank') <= 5)
top_five_in_each_season.show()

from pyspark.sql import functions as F
import matplotlib.pyplot as plt
#Whether fire type calls are seasonal?

monthly_call_counts = clean_df.groupBy('Year', 'Season').count().orderBy('Year', 'Season')
monthly_call_counts_pd = monthly_call_counts.toPandas()
plt.figure(figsize=(12, 6))
plt.plot(monthly_call_counts_pd['Year'].astype(str) + '-' + monthly_call_counts_pd['Season'].astype(str), monthly_call_counts_pd['count'], marker='o')
plt.title('Monthly Fire Call Counts Over Time')
plt.xlabel('Year-Season')
plt.ylabel('Call Count')
plt.xticks(rotation=45)
plt.grid()
plt.show()

filtered_data = clean_df.filter(col('Year') == 2018)
monthly_call_counts = filtered_data.groupBy('Month').count()
max_calls = monthly_call_counts.agg({"count": "max"}).collect()[0][0]
top_months = monthly_call_counts.filter(col("count") == max_calls)
top_months.show()

yearly_call_counts = clean_df.groupBy('Year', 'CallType').count()
major_call_type_by_year = yearly_call_counts.groupBy('Year').agg(max('count').alias('MaxCount'))
major_call_type_details = yearly_call_counts.join(
    major_call_type_by_year,
    (yearly_call_counts['Year'] == major_call_type_by_year['Year']) & (yearly_call_counts['count'] == major_call_type_by_year['MaxCount']),
    'inner'
)
result = major_call_type_details.select(col('Year'), 'CallType', 'MaxCount')
result.show()

average_delay_by_call_type = clean_df.groupBy('CallType').agg(avg('Delay').alias('AverageDelay'))
average_delay_by_call_type.show()

#Find which calltype has maximum average delay time.
average_delay_by_call_type = clean_df.groupBy('CallType').agg(avg('Delay').alias('AverageDelay'))
max_average_delay_call_type = average_delay_by_call_type.orderBy(col('AverageDelay').desc()).first()
print("Call Type with Maximum Average Delay:")
print("Call Type:", max_average_delay_call_type['CallType'])
print("Average Delay:", max_average_delay_call_type['AverageDelay'])

#Which neighborhood in San Francisco generated the most fire calls in 2018
neighborhood_call_counts = clean_df.filter((col('Year') == 2018) & (col('Neighborhood').isNotNull())) \
    .groupBy('Neighborhood').count()
max_calls_neighborhood = neighborhood_call_counts.orderBy(col('count').desc()).first()
print("Neighborhood:", max_calls_neighborhood['Neighborhood'])
print("Number of Fire Calls:", max_calls_neighborhood['count'])

#Which neighborhoods had the worst response times to fire calls in 2018?
filtered_data = clean_df.filter((col('Year') == 2018) & (col('Delay').isNotNull()))
neighborhood_response_time = filtered_data.groupBy('Neighborhood').agg(avg('Delay').alias('AverageResponseTime'))
worst_response_time_neighborhoods = neighborhood_response_time.orderBy(col('AverageResponseTime').desc())
worst_response_time_neighborhoods.show()

#Find out calltype whose average response delay time is maximum, increases, decreases or has no relation over years
average_delay_by_year_calltype = clean_df.groupBy('Year', 'CallType').agg(avg('Delay').alias('AverageDelay'))
pivoted_data = average_delay_by_year_calltype.groupBy('Year').pivot('CallType').agg(avg('AverageDelay')).na.fill(0)
data = pivoted_data.select('Year', 'CallType').orderBy('Year').toPandas()
plt.plot(data['Year'], data['CallType'])
plt.title('Trend of Average Delay for TypeA Over Years')
plt.xlabel('Year')
plt.ylabel('Average Delay')
plt.show()

#For each year find out which city has more calltype

call_type_by_city_year = clean_df.groupBy('Year', 'CallType', 'City').count()
window_spec = Window.partitionBy('Year', 'CallType').orderBy(col('count').desc())
max_call_type_by_city_year = call_type_by_city_year.withColumn('rank', rank().over(window_spec))
max_calls_by_city_year = max_call_type_by_city_year.filter(col('rank') == 1)
max_calls_by_city_year.show()

#For each year find out which city has more calltype

call_type_by_city_year = clean_df.groupBy('Year', 'CallType', 'City').count()
window_spec = Window.partitionBy('Year', 'CallType').orderBy(col('count').desc())
max_call_type_by_city_year = call_type_by_city_year.withColumn('rank', rank().over(window_spec))
max_calls_by_city_year = max_call_type_by_city_year.filter(col('rank') == 1)
max_calls_by_city_year.show()

#Is there a correlation between neighborhood, zip code, and number of fire calls
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import DoubleType
from pyspark.ml.stat import Correlation
from pyspark.ml.feature import VectorAssembler

# Create a Spark session
aggregated_data = clean_df.groupBy('Neighborhood', 'ZipCode').count()
pivoted_data = aggregated_data.groupBy('Neighborhood').pivot('ZipCode').count().na.fill(0)
assembler = VectorAssembler(inputCols=pivoted_data.columns[1:], outputCol="features")
feature_vector = assembler.transform(pivoted_data)
correlation_matrix = Correlation.corr(feature_vector, "features", method="pearson")
correlation_array = correlation_matrix.collect()[0]["pearson(features)"].toArray()
for i, neighborhood in enumerate(pivoted_data.select('Neighborhood').rdd.map(lambda x: x[0]).collect()):
    for j, zip_code in enumerate(pivoted_data.columns[1:]):
        correlation = correlation_array[i][j]
        print(f"Correlation between {neighborhood} and {zip_code}: {correlation}")













